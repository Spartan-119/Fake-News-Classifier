{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4q973jpAk7C",
        "outputId": "6d45e1d3-25a4-4d79-82b0-c66c2bb5398e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "!pip install torchwordemb\n",
        "!pip install -U bcolz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchwordemb\n",
            "  Downloading https://files.pythonhosted.org/packages/36/8d/325faeadd11dd20a908899821070692edfa4d375761a56b975cc6f75be6b/torchwordemb-0.0.9.tar.gz\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from torchwordemb) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchwordemb) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->torchwordemb) (0.16.0)\n",
            "Building wheels for collected packages: torchwordemb\n",
            "  Building wheel for torchwordemb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchwordemb: filename=torchwordemb-0.0.9-cp36-cp36m-linux_x86_64.whl size=1787602 sha256=fe839cb9970afab05ff961bb342a11aeb6a86c06f650efc317c5e7e32c5a9b22\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/e1/89/681d1be47271318455f36c2845f99cc33757dda3fc8b6731c7\n",
            "Successfully built torchwordemb\n",
            "Installing collected packages: torchwordemb\n",
            "Successfully installed torchwordemb-0.0.9\n",
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.18.4)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp36-cp36m-linux_x86_64.whl size=2656139 sha256=613b3b178b8e6f31f9c866679cc2cd3e515aee09da68359b8bb12d3c5e28f5a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch2ToLWoA9dn"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZDnLB5rcCsS"
      },
      "source": [
        "Source Code for : Fact verification, Using pytorch, the architecture of the neural network is taken from \"fact_iiith\" paper  (see whatsapp group), INCOMPLETE ATM , CODE WILL NOT WORK ATM!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhlFs1KQVxVf"
      },
      "source": [
        "ARCHITECTURE THAT IS BEING IMPLEMENTED IS AS FOLLOW : \n",
        "![alt text](https://drive.google.com/uc?id=1FxQJc8wv8K2PtQSj_kkNMljI1nM_BnaR)\n",
        "\n",
        "Both the claim & Evidence are passed through same embedding-layer, this provide us with D x N and D x M matrices. Note that D is the number of dimensions for each word. This value is 50 for glove dataset we used.\n",
        "\n",
        "lets take up any 1 of these matrix and see how further operations are performed \n",
        "\n",
        "1. Each column in matrix represent an vector which is numerical representation of corresponding word in sentence\n",
        "2. Each vector is of same size ( D ) \n",
        "3. We take these vectors 1 by 1 and pass them to LSTM Layer\n",
        "4. Final output of LSTM is of size D (an array/vector of size D)\n",
        "5. this output is merged with another output produced by different LSTM network using element wise multiplication\n",
        "\n",
        "6. the final 1 X D output (or array of size D) is fed into a simple fully connected neural network which takes in input of size D and produces output of size 3 ( TRUE, FALSE, NOT ENOUGH INFO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9ctm2PMP1OM",
        "outputId": "78013b20-0709-4a7a-c09b-0146146623af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# INITIAL SET-UP, SOME OF THE MODULES IMPORTED ARE NOT REQUIRED AND WILL BE REMOVED LATER ON \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import re #regular exp.\n",
        "import string\n",
        "import pickle\n",
        "import gensim\n",
        "import nltk\n",
        "\n",
        "import torchtext as tt\n",
        "import torchwordemb\n",
        "import sys\n",
        "import bcolz\n",
        "from os import path\n",
        "\n",
        "#pytorch modules (pytorch.org)\n",
        "import torch\n",
        "from torch.nn.modules import Module\n",
        "from torch.nn import LSTM\n",
        "from torch.nn import Embedding\n",
        "from torch.nn import Sigmoid\n",
        "from torch.nn import Dropout\n",
        "from torch.utils.data import Dataset, DataLoader #an abstract class \n",
        "\n",
        "from string import punctuation # all possible punctuations, this is imported because we will be removing all punctuations from our sentences\n",
        "punctuation = list(punctuation+ \"-\")\n",
        "print(punctuation)\n",
        "punctuation_map = dict()\n",
        "for ch in punctuation:\n",
        "  punctuation_map[ch]=0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '-']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxzN0BSPkriH"
      },
      "source": [
        "Now that we are done importing modules, we will first implement a custom pytorch dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RJLH_n9ixDf",
        "outputId": "5f39afec-90b4-452b-d254-da440d64ecd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "#Create Custom Dataset (refer official doc @ https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) \n",
        "\n",
        "#Dataset will consist of \n",
        "    # 1. training_dataFrame (df) : this is dataframe where all data is stored in tabular format (3 columns)\n",
        "    # 2. vocab_words (list of str) : list of all possible distinct words used\n",
        "def getDistinctWordSet(userString):\n",
        "  return set(userString.split(' '))\n",
        "\n",
        "def removePunctuation(sent):\n",
        "  for c in punctuation:\n",
        "    if(c in sent):\n",
        "      punctuation_map[c]+=1\n",
        "      sent = sent.replace(c,' ')\n",
        "  return sent\n",
        "\n",
        "def print_punctuation_counts():\n",
        "  for ch in punctuation_map:\n",
        "    print(ch +\" : \"+str(punctuation_map[ch]))\n",
        "\n",
        "def reset_punctuation_map():\n",
        "  for ch in punctuation_map:\n",
        "    punctuation_map[ch]=0\n",
        "\n",
        "class FEVER_Dataset(Dataset):\n",
        "  \n",
        "  #OUTPUT LABEL LABEL MEANING\n",
        "  # 0 => TRUE/positive\n",
        "  # 1 => FALSE/negative\n",
        "  # 2 => NEI (Not enough info)\n",
        "\n",
        "  def __init__(self): #loads data\n",
        "\n",
        "    try:\n",
        "      data_temp = (pd.read_json('/content/drive/My Drive/Colab Notebooks/CLAIM_SENTENCE_LABEL.json'))\n",
        "      print(\"DATASET LOADED\")\n",
        "    except:\n",
        "      raise Exception(\"FEVER_DATASET ERROR : Cannot find claim_setence_label.json in content/\")\n",
        "\n",
        "    self.training_dataFrame = data_temp.loc[:,['claim', 'sentence', 'label']]\n",
        "\n",
        "    print(self.training_dataFrame.head())\n",
        "    print(\"type of training_dataFrame \"+ str(type(self.training_dataFrame)))\n",
        "    \n",
        "    #--------------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    print(\"-------DATASET PREPROCESSING BEGINS--------\")    \n",
        "    print(\"  >> datatype : \\n\"+str(self.training_dataFrame.dtypes)) #passed, datatype is good by default\n",
        "\n",
        "    claim_null=0\n",
        "    sentence_null=0\n",
        "    for i in range(self.training_dataFrame.shape[0]):\n",
        "      if not self[i]['claim']:\n",
        "        claim_null+=1\n",
        "      if not self[i]['sentence']:\n",
        "        sentence_null+=1\n",
        "    print(\"total null (claim + sentence null): \" + str(claim_null+sentence_null)) #passed, no empty strings\n",
        "\n",
        "    #output label distribution\n",
        "    counter = ((self.training_dataFrame.label == 0).sum(), (self.training_dataFrame.label == 1).sum(), (self.training_dataFrame.label == 2).sum())\n",
        "    print(\"distribution of output classes : \" + str(counter))\n",
        "\n",
        "    #plot graph\n",
        "    fig, ax = plt.subplots()\n",
        "    y_pos = np.arange(len(('True', 'False', 'NotSure')))\n",
        "    ax.barh(y_pos, counter, align='center')\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(('True', 'False', 'NotSure'))\n",
        "    #ax.invert_yaxis()\n",
        "    ax.set_xlabel('Counts')\n",
        "    ax.set_title('distribution of label')\n",
        "    plt.show()\n",
        "\n",
        "    #convert everything to lowercase\n",
        "    self.training_dataFrame.loc[:,['claim']] = self[:]['claim'].str.lower()\n",
        "    self.training_dataFrame.loc[:,['sentence']]=self[:]['sentence'].str.lower()\n",
        "\n",
        "    claim_with_punctuation=0\n",
        "    sentence_with_punctuation=0\n",
        "    \n",
        "    #label sentences with punctuation\n",
        "    temp_i=0\n",
        "    for sent in self[:]['claim']:\n",
        "      new_sent = removePunctuation(sent)\n",
        "      if(new_sent != sent):\n",
        "          claim_with_punctuation+=1\n",
        "      self.training_dataFrame.loc[[temp_i],['claim']] = new_sent\n",
        "      if(temp_i == 2):\n",
        "        print('what is in db : '+str(self[temp_i]['claim']))\n",
        "        print('actually its : ' +new_sent)\n",
        "      temp_i+=1\n",
        "\n",
        "    #print_punctuation_counts()\n",
        "    reset_punctuation_map()\n",
        "\n",
        "    #print(\"-----sentence----\")\n",
        "    temp_i=0\n",
        "    for sent in self[:]['sentence']:\n",
        "      new_sent = removePunctuation(sent)\n",
        "      if(new_sent != sent):\n",
        "        sentence_with_punctuation+=1\n",
        "      self.training_dataFrame.loc[[temp_i],['sentence']] = new_sent \n",
        "      temp_i+=1\n",
        "    \n",
        "    #print(\"claim with punctuation (before) : \"+str(claim_with_punctuation))\n",
        "    #print(\"sentence with punctuation (before) : \"+str(sentence_with_punctuation))  \n",
        "\n",
        "\n",
        "    self.max_claim_length=0\n",
        "    self.max_sentence_length=0\n",
        "    c=''\n",
        "    s=''\n",
        "    #now both claim sentence are clean, therefore we will find maximum of each\n",
        "    for sent in self[:]['claim']:\n",
        "      if len(sent.split()) > self.max_claim_length:\n",
        "        self.max_claim_length = len(sent.split())\n",
        "        c = sent\n",
        "  \n",
        "    for sent in self[:]['sentence']:\n",
        "      if len(sent.split()) > self.max_sentence_length:\n",
        "        self.max_sentence_length = len(sent.split())\n",
        "        s = sent\n",
        "      \n",
        "    print(c)\n",
        "    print(s)   \n",
        "    print(\"max claim length =  \" + str(self.max_claim_length) + \"and sent length = \" + str(self.max_sentence_length))\n",
        "  #---------------------------------------------------------------------------------------\n",
        "  \n",
        "  def getVocabSet(self):\n",
        "    vocabSet = set()\n",
        "    for i in range(self.training_dataFrame.shape[0]):\n",
        "      c = self.getClaim(i)\n",
        "      s = self.getSentence(i)\n",
        "      vocabSet = vocabSet | getDistinctWordSet(c) | getDistinctWordSet(s)\n",
        "    return vocabSet\n",
        "\n",
        "  def __getitem__(self, index): #Please note that do not use the result obtained thru this method to update training_dataFrame, use self.training_dataFrame.loc \n",
        "    obj_dict = {}\n",
        "    obj_dict['claim'] = self.training_dataFrame.loc[index]['claim']\n",
        "    obj_dict['sentence'] = self.training_dataFrame.loc[index]['sentence']\n",
        "    obj_dict['label'] = self.training_dataFrame.loc[index]['label']\n",
        "    return obj_dict#self.training_dataFrame.loc[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(self.training_dataFrame.shape[0])\n",
        "\n",
        "  def getClaim(self, index):\n",
        "    obj = self[index]['claim']\n",
        "    return obj\n",
        "\n",
        "  def getSentence(self, index):\n",
        "    obj = self[index]['sentence']\n",
        "    return obj\n",
        "\n",
        "  def getLabel(self, index):  \n",
        "    return self[index]['label']\n",
        "\n",
        "  def getData(self):\n",
        "    return self.training_dataFrame\n",
        "#end of class \n",
        "\n",
        "obj_dataset = FEVER_Dataset()\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(obj_dataset, [7000, 1412]) #split dataset\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size = 64)\n",
        "test_loader = DataLoader(test_set, batch_size = 64)\n",
        "\n",
        "print(\"type : \"+str(type(train_set)))\n",
        "print(\"Number of distinct Words (vocab size): \"+ str(len(obj_dataset.getVocabSet())))\n",
        "print(\"Shape : \" + str(obj_dataset.training_dataFrame.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATASET LOADED\n",
            "                                               claim  ... label\n",
            "0  The Black Dahlia was only written by Hillary C...  ...     2\n",
            "1  Dennis Hopper did not act in an American telev...  ...     1\n",
            "2                   Bal Gangadhar Tilak was a judge.  ...     1\n",
            "3  Luke Evans took a five-year acting hiatus from...  ...     1\n",
            "4               Colin Quinn's middle name is Edward.  ...     0\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "type of training_dataFrame <class 'pandas.core.frame.DataFrame'>\n",
            "-------DATASET PREPROCESSING BEGINS--------\n",
            "  >> datatype : \n",
            "claim       object\n",
            "sentence    object\n",
            "label        int64\n",
            "dtype: object\n",
            "total null (claim + sentence null): 0\n",
            "distribution of output classes : (2761, 2847, 2804)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "what is in db : bal gangadhar tilak was a judge \n",
            "actually its : bal gangadhar tilak was a judge \n",
            "international relations includes technology and engineering  economics  communication studies  history  international law  demography  philosophy  geography  social work  sociology  anthropology  criminology  psychology  gender studies  cultural studies  culturology  and diplomacy  globalization  diplomatic relations  state sovereignty  international security  ecological sustainability  nuclear proliferation  nationalism  economic development  global finance  as well as terrorism and organized crime  human security  foreign interventionism  and human rights  as well  as  more recently  comparative religion \n",
            "he has voiced joel in the last of us   booker dewitt in bioshock infinite   rhys in tales from the borderlands   delsin rowe in infamous second son   the joker in batman   arkham origins and batman   assault on arkham   talion in middle earth   shadow of mordor   kai leng in mass effect 3   jack mitchell in call of duty   advanced warfare   vincent brooks in catherine   yuri lowell in tales of vesperia   kanji tatsumi in shin megami tensei   persona 4   snow in the final fantasy xiii series   pagan min in far cry 4   gul   dan in world of warcraft   warlords of draenor   sam drake in uncharted 4   a thief  s end and bruce wayne in batman   the telltale series  \n",
            "max claim length =  65and sent length = 115\n",
            "type : <class 'torch.utils.data.dataset.Subset'>\n",
            "Number of distinct Words (vocab size): 17529\n",
            "Shape : (8412, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_1jg4yltgx_",
        "outputId": "8e125bec-87a8-42a3-f648-b8fb40f75bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "for d in train_loader:\n",
        "  c, s, l = d\n",
        "  print(type(d['claim']))\n",
        "  print(type(d['sentence']))\n",
        "  print(type(d['label']))\n",
        "  break \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLOs3_yDyJIO"
      },
      "source": [
        "#LOAD TRAIN_SET AND TEST_SET from google drive\n",
        "path_trainSet = \"/content/drive/My Drive/Colab Notebooks/training_set\"\n",
        "path_testSet = \"/content/drive/My Drive/Colab Notebooks/test_set\"\n",
        "\n",
        "with open(path_trainSet, 'rb') as f:\n",
        "  train_set = pickle.load(f)\n",
        "\n",
        "with open(path_testSet, 'rb') as f:\n",
        "  test_set = pickle.load(f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3nwHl26jDGl",
        "outputId": "f9240d15-8da1-4c6f-f38b-49779522c62a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#CHECKING DISTRIBUTION OF RECORDS IN TRAIN AND TEST SET \n",
        "distribution = [0, 0, 0]\n",
        "for i in range(len(train_set)):\n",
        "  distribution[train_set[i].label] += 1\n",
        "print(\"STATISTICS FOR TRAIN_SET : \" + str(distribution))\n",
        "distribution = [0, 0, 0]\n",
        "for i in range(len(test_set)):\n",
        "  distribution[test_set[i].label] += 1\n",
        "print(\"STATISTICS FOR TEST_SET : \"+ str(distribution))  \n",
        "\n",
        "type(train_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STATISTICS FOR TRAIN_SET : [2298, 2362, 2340]\n",
            "STATISTICS FOR TEST_SET : [463, 485, 464]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.Subset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DocVtuW3uuR1"
      },
      "source": [
        "#SAVE TRAIN SET AND TEST-SET in google drive\n",
        "with open(path_trainSet, 'wb') as f:\n",
        "    pickle.dump(train_set, f)\n",
        "\n",
        "with open(path_testSet, 'wb') as f:\n",
        "    pickle.dump(test_set, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1MMpzsTk0n3"
      },
      "source": [
        "Before we move any further, we need to implement an embedding layer, since training embedding layer is going to be tough, im using pretrained embedding layer. The code mentioned below is taken from website as mentioned below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4qrS9zWqI9s"
      },
      "source": [
        "#This class is responsible for setting up pre-trained embeddings, its constructor \n",
        "#accepts an input called embedding_type, \n",
        "#embedding_type = 0 => google news word2vec embedding \n",
        "#embedding_type = 1 => glove embedding\n",
        "\n",
        "class PretrainedEmbeddingSetup:\n",
        "  def __init__(self, embedding_type, already_executed_before=True): #0 => oogle news word2ve embedding, 1 => glove\n",
        "    self.embedding_used = embedding_type\n",
        "    if(embedding_type<0 or embedding_type>1):\n",
        "      print(\"error, invalid embeding_type. Use 0 for word2vec and 1 for glove\")\n",
        "      return;\n",
        "    \n",
        "    self.path_to_read_raw_dataset = '/content/drive/My Drive/Colab Notebooks' \n",
        "    self.output_path = '/content/drive/My Drive/Colab Notebooks/parsedEmbeddingOutput'\n",
        "\n",
        "    # gemsim KeyedVectors model ke bare mai yahan se dekhlo : https://radimrehurek.com/gensim/models/keyedvectors.html\n",
        "    if(embedding_type==0): #prepare Google News word2vec\n",
        "      print(\"preparing google-news word2vec embedding layer\")\n",
        "      \n",
        "      if(already_executed_before == False):\n",
        "        print('executing code first time : preparing embedding from scratch')\n",
        "\n",
        "        #read binary file containg [word]-[vec] pair in each line\n",
        "        GensimModel = gensim.models.KeyedVectors.load_word2vec_format(self.path_to_read_raw_dataset+'/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "      \n",
        "\n",
        "        vectors = bcolz.carray(GensimModel.vectors, rootdir=f''+self.output_path+'/google_w2v.dat', mode='w')\n",
        "       \n",
        "\n",
        "        vectors = bcolz.carray(vectors[0:].reshape((-1, 300)), rootdir=f''+self.output_path+'/google_w2v.dat', mode='w')\n",
        "        vectors.flush()\n",
        "\n",
        "        self.word2idx = {}\n",
        "        self.word2idx = {word: i for i, word in enumerate(GensimModel.index2word)}\n",
        "         \n",
        "        GensimModel=0 \n",
        "        import gc\n",
        "        gc.collect()\n",
        "\n",
        "        with open(self.output_path+'/word2idx', 'wb') as f:\n",
        "          pickle.dump(self.word2idx, f)\n",
        "  \n",
        "      else:\n",
        "        self.word2idx = {}\n",
        "        with open(self.output_path+'/word2idx', 'rb') as f:\n",
        "          self.word2idx = pickle.load(f)\n",
        "        print('loaded word2idx, type = '+str(type(self.word2idx)))\n",
        "\n",
        "      #word2idx word \n",
        "      self.vectors = bcolz.open(f''+self.output_path+'/google_w2v.dat', mode='r')\n",
        "\n",
        "      GensimModel=0 \n",
        "      import gc\n",
        "      gc.collect()  #forcing garbage collection to save up space occupied by gensimModel keyedVector model\n",
        "      print(\"WORD2VEC EMBEDDING SUCCESSFULLY IMPORTED\")\n",
        "\n",
        "    else:\n",
        "      print(\"preparing glove embedding layer\")\n",
        "      #Load pretrained embedding layer (ref : \"medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\")\n",
        "      words=[]\n",
        "      word2idx={}\n",
        "      #prepare ondisk container, initially we declare an array\n",
        "      #this file will be written to specific location (output_glove_path) on disk\n",
        "      vectors = bcolz.carray(np.zeros(1), rootdir=f''+self.output_path+'/6B.50.dat', mode='w')\n",
        "      #Parse the glove file, which contains data in format <word> <array of size 50 containing learnt representn>\n",
        "      #we will store all words in array word, and also store their index in word2idx array, \n",
        "      # we can use word2idx value to find corresponding vector associated with word in \"vectors\" array \n",
        "      idx = 0\n",
        "      with open(f''+self.path_to_read_raw_dataset+'/glove.6B.50d.txt', 'rb') as f:\n",
        "        for l in f: #extracting each line in file \n",
        "          line = l.decode().split() #break line based on \" \"\n",
        "          word = line[0] #first,\n",
        "          words.append(word)\n",
        "          word2idx[word]=idx #map word to index , as it appear in the glove file \n",
        "          idx += 1\n",
        "          vect = np.array(line[1:]).astype(np.float) #from index 1 till end, use these floating point nums. to build vector\n",
        "          vectors.append(vect)\n",
        "\n",
        "      #parsing is completed, write the result back into the file\n",
        "      vectors = bcolz.carray(vectors[1:].reshape((-1, 50)), rootdir=f''+self.output_path+'/6B.50.dat', mode='w')\n",
        "      vectors.flush()\n",
        "\n",
        "      #pickle is inbuilt python lib. for serializing and deserializing python objects \n",
        "\n",
        "      #dump(obj, file) > Write the pickled representation of the object obj to the open file object file\n",
        "      pickle.dump(words, open(f''+self.output_path+'/6B.50_words.pkl', 'wb')) #writing words data to disk\n",
        "      pickle.dump(word2idx, open(f''+self.output_path+'/6B.50_idx.pkl', 'wb')) #writing index to disk\n",
        "\n",
        "      #finally building embedding dictionary\n",
        "      vectors = bcolz.open(f''+self.output_path+'/6B.50.dat')#[:]\n",
        "\n",
        "      #picke.load() => Read the pickled representation of an object from the open file object file and return the reconstituted object hierarchy specified therein\n",
        "      words = pickle.load(open(f''+self.output_path+'/6B.50_words.pkl', 'rb'))\n",
        "      word2idx = pickle.load(open(f''+self.output_path+'/6B.50_idx.pkl', 'rb'))\n",
        "\n",
        "      #glove maps word to its embedding vector, total number of words => 4,00,000\n",
        "      self.glove = {w: vectors[word2idx[w]] for w in words} \n",
        "    \n",
        "      #usage : glove['word']\n",
        "      #print(glove['the'])\n",
        "      print(\"GLOVE SUCCESSFULLY INITIALISED. USAGE : getVector(word)\")\n",
        "\n",
        "  def getEmbeddingVector(self, sampleWord):\n",
        "    if(self.embedding_used==0):\n",
        "      return self.vectors[self.word2idx[sampleWord]]\n",
        "    else:\n",
        "      return self.glove[sampleWord];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8NfJmYM1vC4"
      },
      "source": [
        "\n",
        "\n",
        "#now we have embeddings for 400k words, lets build a function that will return a pre-trained embeddin layer\n",
        "#an embedding layer takes input a number (index of word) and returns the vector associated with that number\n",
        "def createEmbeddingLayer(weight_matrix, non_trainable=True):\n",
        "  #this fxn should be called while building network\n",
        "  weight_matrix = torch.Tensor(weight_matrix)\n",
        "  num_embeddings, embedding_dim = weight_matrix.size()\n",
        "  emb_layer = Embedding(num_embeddings, embedding_dim)\n",
        "  emb_layer.load_state_dict({'weight': weight_matrix})\n",
        "  if non_trainable:\n",
        "    emb_layer.weight.requires_grad = False\n",
        "  \n",
        "  return emb_layer, num_embeddings, embedding_dim \n",
        "\n",
        "#some quick notes for WEIGHT_MATRIX the shape is as follow\n",
        "# Vocab_Size X Output_Dimension (ROWS X COLUMN) : INPUT DIMENSION\n",
        "# Vocab_Size => count of all possible words expected\n",
        "# Output_Dimension => 50 (we are producing embedding vector of len 50)\n",
        "\n",
        "#How to get Vocab_Size ? \n",
        "# for each word in our training dataset , check if corresponding weight-vector exist in pretrained model\n",
        "# if exist => use it \n",
        "# else initialize some random vector (ADDS TO ERROR IN ACCURACY)\n",
        "\n",
        "#this function inputs 64 CLAIMS OR 64 SENTENCE, \n",
        "#input = batch_size x variable_list_size (basically 64 \"str\" types, 1 in each row and therefore total 64 rows)\n",
        "#O/P FORMAT : (batch_size x fixed_padding_size)\n",
        "def getBatchIndexTensor(net, batched_wordList, isClaim=True): #ip is array of string\n",
        "  indexBatchlist=[]\n",
        "\n",
        "  for text in batched_wordList:\n",
        "    tokenList = text.split() #creates list of words / tokens\n",
        "    paddedTokenTensor = getPaddedIndexTensor(net, tokenList, isClaim)\n",
        "    indexBatchlist.append(paddedTokenTensor) \n",
        "  \n",
        "  output_tensor = torch.stack(indexBatchlist)\n",
        "  return output_tensor\n",
        "\n",
        "#same as getIndexTensor except returns padded tensor\n",
        "def getPaddedIndexTensor(net, wordList=[], isClaim=True):\n",
        "  indexList = []\n",
        "  for word in wordList:\n",
        "    if word in net.word2index:\n",
        "      indexList.append(int(net.word2index[word]))\n",
        "    else:\n",
        "      indexList.append(int(net.OOV_Index))\n",
        "  \n",
        "  #padd extra 0s\n",
        "  if isClaim == True:\n",
        "    zeroes_to_add = obj_dataset.max_claim_length - len(indexList)\n",
        "    indexList.extend([0] * zeroes_to_add)\n",
        "\n",
        "  elif isClaim == False:\n",
        "    zeroes_to_add = obj_dataset.max_sentence_length - len(indexList)\n",
        "    indexList.extend([0] * zeroes_to_add)\n",
        "\n",
        "  #padded\n",
        "  output = torch.tensor(indexList, dtype=torch.long)\n",
        "  return output\n",
        "\n",
        "\n",
        "#this function take input either 1 \"claim\" or 1 \"sentence\" in form of tokens (list)\n",
        "def getIndexTensor(net, wordList=[]):\n",
        "  indexList = []\n",
        "  for word in wordList:\n",
        "    if word in net.word2index:\n",
        "      indexList.append(int(net.word2index[word]))\n",
        "    else:\n",
        "      indexList.append(int(net.OOV_Index))\n",
        "  output = torch.tensor(indexList, dtype=torch.long)\n",
        "  return output\n",
        "\n",
        "\n",
        "#This class implements entire Neural Network, note that the objective atm is to implement a working prototype, Accuracy is ignored \n",
        "#at the moment, The network accepts 2 lists (claim & evidence) , each consisting of tokens however order of these tokens is preserved.  \n",
        "# both these inputs are then passed through same embedding layer to convert them into numerical representation, the output of embedding layer \n",
        "# is a 2D Matrix (dimensions will be declared later when more concrete implementation is done)\n",
        "# these two 2D Matrices are then fed to respective LSTM network, the output is merged (element-multiplication) \n",
        "# this merged output is finally passed through a general feedforward network which gives us the final output\n",
        "\n",
        "\n",
        "# 1. Accept Claim and Evidence as input \n",
        "class Network(Module):\n",
        "  def __init__(self, embedding_to_use):# if embedding_to_use is 0 it means we are going to use word2vec(best choice), if 1 => glove (not best)\n",
        "    super(Network, self).__init__()\n",
        "    print(\"Network Constructor called > loading pretrained embedding first\")\n",
        "    \n",
        "    pretrained_embedding = PretrainedEmbeddingSetup(embedding_type=embedding_to_use)\n",
        "    embedding_dimension = 0\n",
        "    if(embedding_to_use == 0): #word2vec\n",
        "      embedding_dimension = 300\n",
        "    elif(embedding_to_use == 1): #\n",
        "      embedding_dimension = 50\n",
        "\n",
        "    #embed layer : CREATE WEIGHT MATRIX (PRETRAINED) \n",
        "    self.vocabSet = obj_dataset.getVocabSet() #get all words in our training dataset\n",
        "    matrix_length = len(self.vocabSet)+1 #+1 because we are reserving 1 index for OOV \n",
        "    weight_matrix = np.zeros((matrix_length, embedding_dimension)) #init weight matrix that we will use\n",
        "    word_found=0\n",
        "    random_word_generated=0\n",
        "    self.word2index ={} #this dictionary is what we will use to process input\n",
        "    for i, word in enumerate(self.vocabSet):\n",
        "      self.word2index[word]=i\n",
        "      try:\n",
        "        weight_matrix[i]=pretrained_embedding.getEmbeddingVector(word)\n",
        "        word_found+=1\n",
        "      except KeyError:\n",
        "        random_word_generated+=1\n",
        "        #print(\"not found : \"+word)\n",
        "        weight_matrix[i]=np.random.normal(scale=0.6, size=(embedding_dimension,))\n",
        "\n",
        "    #average of all, note that last index of weight matrix returns embedding for OOV words (out of vocab)\n",
        "    #this is just average of all vocab words (ref : https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g--6b-50d-txt)\n",
        "\n",
        "    self.OOV_Index = weight_matrix.shape[0]-1;glove\n",
        "    if(embedding_to_use == 0): #word2vec\n",
        "      weight_matrix[self.OOV_Index] = np.zeros((300,));\n",
        "    elif(embedding_to_use == 1):\n",
        "      weight_matrix[self.OOV_Index] = [-0.12920076, -0.28866628, -0.01224866, -0.05676644, -0.20210965, -0.08389011, 0.33359843,  0.16045167,  0.03867431,  0.17833012,  0.04696583, -0.00285802, 0.29099807,  0.04613704, -0.20923874, -0.06613114, -0.06822549,  0.07665912, 0.3134014, 0.17848536, -0.1225775, -0.09916984, -0.07495987,  0.06413227, 0.14441176,  0.60894334,  0.17463093,  0.05335403, -0.01273871 , 0.03474107, -0.8123879 , -0.04688699,  0.20193407,  0.2031118 , -0.03935686,  0.06967544, -0.01553638 ,-0.03405238, -0.06528071 , 0.12250231,  0.13991883, -0.17446303, -0.08011883,  0.0849521,  -0.01041659, -0.13705009,  0.20127155,  0.10069408, 0.00653003,  0.01685157];\n",
        "    else:\n",
        "      print(\"ERROR\")\n",
        "\n",
        "    print(weight_matrix.shape)\n",
        "    print(\"Total Words Matched with Pretrained Dataset : \"+str(word_found))\n",
        "    print(\"Total Words for which random Embedding generated : \"+str(random_word_generated))\n",
        "    \n",
        "    #print(\"index of (the) : \"+str(self.word2index['the']))\n",
        "    #print('vector value as per weight matrix : ' + str(weight_matrix[self.word2index['the']]))\n",
        "    #print('vector value as per glove ' + str(glove['the']))\n",
        "\n",
        "    self.embedding, num_embeddings, embedding_dim = createEmbeddingLayer(weight_matrix, True)\n",
        "    \n",
        "    lstm_layers = 2\n",
        "    lstm_hidden_size = 100\n",
        "    #output and hidden size are same thing (STACKED , BIDIRECTIONAL LSTM) , i changd hidden_size to 100 from 10\n",
        "    self.lstm_claim = LSTM(num_layers=lstm_layers, input_size=embedding_dimension, hidden_size=lstm_hidden_size, bidirectional=True)\n",
        "    \n",
        "\n",
        "    self.lstm_claim_denseNN = torch.nn.Sequential(\n",
        "        torch.nn.Dropout(),\n",
        "        torch.nn.Linear(lstm_layers*lstm_hidden_size, 100),\n",
        "        torch.nn.Sigmoid()\n",
        "    )\n",
        "    \n",
        "    self.lstm_evidence = LSTM(num_layers=lstm_layers, input_size=embedding_dimension, hidden_size=lstm_hidden_size, bidirectional=True)\n",
        "    \n",
        "    \n",
        "    self.lstm_evidence_denseNN = torch.nn.Sequential(\n",
        "        torch.nn.Dropout(),\n",
        "        torch.nn.Linear(lstm_layers*lstm_hidden_size, 100),\n",
        "        torch.nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.outputLayer = torch.nn.Sequential(\n",
        "        torch.nn.Linear(100, 3), #Linear(inputshape, output shape)\n",
        "        torch.nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  #use getIndexTensor(model, tokenList) to generate claimIndexTensor and sentenceIndexTensor during call to model\n",
        "  def forward(self, claimIndexTensor, sentenceIndexTensor):\n",
        "\n",
        "    #input shape to embedding : seq_length(padded) x batch_size(64)\n",
        "    #output shape : seq_length(padded) X batch-size(64) X embedding_dimension(300)\n",
        "    claimIndexTensor = torch.transpose(claimIndexTensor, 0, 1)\n",
        "    sentenceIndexTensor = torch.transpose(sentenceIndexTensor, 0, 1)\n",
        "    #print(\"   claim tensor shape : \" + str(claimIndexTensor.shape))\n",
        "    #print(\"   sentence tensor shape : \" + str(sentenceIndexTensor.shape))\n",
        "    \n",
        "    outputClaim = self.embedding(claimIndexTensor)\n",
        "    outputSentence = self.embedding(sentenceIndexTensor)\n",
        "    \n",
        "    #print(\"embedding output shape \")\n",
        "    #print(\"   claim-tensor output : \"+str(outputClaim.shape)) # N(seq-len) x 64(batch-size) x 300(embed-dimension)\n",
        "    #print(\"   sentence-tensor output : \" + str(outputSentence.shape)) # M x 64 x 300\n",
        "    \n",
        "    \n",
        "    #outputClaim = torch.unsqueeze(outputClaim, 1)\n",
        "    #outputSentence = torch.unsqueeze(outputSentence, 1)\n",
        "    \n",
        "    #input to lstm is of shape (seq_len, batch, input_size), seq_length = size of text, input_size = 300\n",
        "    lstm_claimVec, lstm_claimForwardedPair = self.lstm_claim(outputClaim) #o/p shape for lstm_claimVec = [seq_len(N) X 64 X (2*hidden_size)]\n",
        "    lstm_evidenceVec, lstm_evidenceForwardedPair = self.lstm_evidence(outputSentence) #o/p for lstm_evidenceVec = [seq_len(M) X 64 x (2*hidden_size)]\n",
        "    #LSTM Output Shape :(seq_len, batch, num_directions * hidden_size)\n",
        "    \n",
        "    #IMPORTANT : \n",
        "    #Note that we are concerned with only last output of LSTM, and shape of that output is 2*hidden_size (or 200),\n",
        "    #the last output exist at : [-1::], i.e, for last word of sequences, get output from each batch \n",
        "\n",
        "    #DROP AXIS AT POS 1 (pos 1 as in shape) \n",
        "    #lstm_claimVec = torch.squeeze(lstm_claimVec, 1)\n",
        "    #lstm_evidenceVec = torch.squeeze(lstm_evidenceVec, 1)\n",
        "\n",
        "    final_claim = self.lstm_claim_denseNN(lstm_claimVec[-1::])\n",
        "    final_evidence = self.lstm_evidence_denseNN(lstm_evidenceVec[-1::])\n",
        "    \n",
        "    #print(\"claim-shape before multiplication : \"+str(final_claim.shape))\n",
        "    #print(\"evidence-shape before multiplication : \"+str(final_evidence.shape))\n",
        "    #element wise multiplication\n",
        "    finalRepresentation = (final_claim)*(final_evidence) #finalRepresentation = #self.lstm_claim_denseNN(lstm_claimVec[-1])*self.lstm_evidence_denseNN(lstm_evidenceVec[-1])\n",
        "\n",
        "    #print(\"shape of final multiplication result = \" + str(finalRepresentation.shape))\n",
        "\n",
        "    #print(\"input to FINAL LAYER HAS SHAPE : \"+str(finalRepresentation.shape))\n",
        "    finalOutput = self.outputLayer(finalRepresentation)\n",
        "    #print(\"shape of final layer output = \"  + str(finalOutput.shape))\n",
        "\n",
        "    #finalOutput = finalOutput.reshape(1,3)\n",
        "    #print(\"output shape : \"+str(finalOutput.shape))\n",
        "    return finalOutput\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ldZ1f4qrtSc",
        "outputId": "9eec277d-eb8a-4c88-e0c6-baac3250511a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#for training on gpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atidxPy_Mx5s",
        "outputId": "98931c68-7998-4a4b-c6c8-febc4e575641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "model = Network(embedding_to_use=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network Constructor called > loading pretrained embedding first\n",
            "preparing google-news word2vec embedding layer\n",
            "loaded word2idx, type = <class 'dict'>\n",
            "WORD2VEC EMBEDDING SUCCESSFULLY IMPORTED\n",
            "(17530, 300)\n",
            "Total Words Matched with Pretrained Dataset : 11592\n",
            "Total Words for which random Embedding generated : 5937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g_YtzV1uXTb",
        "outputId": "b177d0eb-9d01-479d-8be9-c70c302e771f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (embedding): Embedding(17530, 300)\n",
              "  (lstm_claim): LSTM(300, 100, num_layers=2, bidirectional=True)\n",
              "  (lstm_claim_denseNN): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=200, out_features=100, bias=True)\n",
              "    (2): Sigmoid()\n",
              "  )\n",
              "  (lstm_evidence): LSTM(300, 100, num_layers=2, bidirectional=True)\n",
              "  (lstm_evidence_denseNN): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=200, out_features=100, bias=True)\n",
              "    (2): Sigmoid()\n",
              "  )\n",
              "  (outputLayer): Sequential(\n",
              "    (0): Linear(in_features=100, out_features=3, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO1myBlnPHk4"
      },
      "source": [
        "#TRAINING AND EVALUATION OF MODEL : ALL VARIABLES INITIALISED HERE\n",
        "import torch.optim as optim\n",
        "\n",
        "LEARNING_RATE = 0.003\n",
        "EPOCH=100\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "GOOGLE_COLAB_BUFFER_SIZE = 5 #how many messages to display on google-colab console \n",
        "bufferSizeCounter=0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prA4UCgaDPTC"
      },
      "source": [
        "#IN CASE MODEL IS TRAINED BEFORE AND YOU NEED TO RESUME TRAINING \n",
        "MODEL_PATH = \"/content/drive/My Drive/Colab Notebooks/savedModel\"\n",
        "\n",
        "ENTIRE_MODEL_PATH = \"/content/drive/My Drive/Colab Notebooks/entire_model\"\n",
        "\n",
        "checkpoint = torch.load(MODEL_PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "model = torch.load(ENTIRE_MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viDbXUapiUpp",
        "outputId": "bb513a39-a17b-4648-d6a5-b1319cbf0f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        }
      },
      "source": [
        "#a new input code for batches\n",
        "#TRAIN THE MODEL\n",
        "model.train()\n",
        "creterion = torch.nn.CrossEntropyLoss() #CrossEntropyLoss automatically applies softmax, and so it takes input the raw_scores of each class\n",
        "running_loss=0\n",
        "LOSS_TRACKER=[]\n",
        "for epoch in range(EPOCH):\n",
        "  running_loss=0\n",
        "  for current_batch, data_in_batch in enumerate(train_loader, 0):\n",
        "    claims_inp = data_in_batch['claim'] #batch_size (64) X seq_size(variable)\n",
        "    sentence_inp = data_in_batch['sentence'] #batch_size (64) x seq_size (variable)\n",
        "    labels_inp = torch.tensor(data_in_batch['label'], dtype=torch.long).to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #prepare inputs for model by converting string array of size batch\n",
        "    claims_inp = getBatchIndexTensor(model, claims_inp, True).to(device)\n",
        "    sentence_inp = getBatchIndexTensor(model, sentence_inp, False).to(device) \n",
        "\n",
        "\n",
        "    local_outputs = torch.squeeze(model(claims_inp, sentence_inp)) #actual output is : 1x64x3, changed to 64x3\n",
        "\n",
        "    #print(\"in-training-loop : shape_of_model_output = \"+str(local_outputs.shape)+\" & shae of labels = \"+str(labels_inp.shape));\n",
        "    loss = creterion(local_outputs, labels_inp)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "  #end of batch loop\n",
        "  LOSS_TRACKER.append(running_loss)\n",
        "  print(LOSS_TRACKER)\n",
        "\n",
        "print(LOSS_TRACKER)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287, 94.1497642993927]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287, 94.1497642993927, 94.22964161634445]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287, 94.1497642993927, 94.22964161634445, 94.21881800889969]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287, 94.1497642993927, 94.22964161634445, 94.21881800889969, 94.22449618577957]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287, 94.1497642993927, 94.22964161634445, 94.21881800889969, 94.22449618577957, 94.24588334560394]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287, 94.1497642993927, 94.22964161634445, 94.21881800889969, 94.22449618577957, 94.24588334560394, 94.22043216228485]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287, 94.1497642993927, 94.22964161634445, 94.21881800889969, 94.22449618577957, 94.24588334560394, 94.22043216228485, 94.25408482551575]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287, 94.1497642993927, 94.22964161634445, 94.21881800889969, 94.22449618577957, 94.24588334560394, 94.22043216228485, 94.25408482551575, 94.25720971822739]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287, 94.1497642993927, 94.22964161634445, 94.21881800889969, 94.22449618577957, 94.24588334560394, 94.22043216228485, 94.25408482551575, 94.25720971822739, 94.2073905467987]\n",
            "[94.193787753582, 94.22347891330719, 94.25284838676453, 94.20452880859375, 94.2359836101532, 94.26536387205124, 94.23885917663574, 94.21735173463821, 94.20355653762817, 94.22536432743073, 94.2202616930008, 94.21656733751297, 94.2406193614006, 94.18864440917969, 94.25550824403763, 94.21291434764862, 94.32234054803848, 94.19734007120132, 94.23383694887161, 94.22931629419327, 94.21582925319672, 94.20347374677658, 94.25822913646698, 94.21951311826706, 94.23303657770157, 94.23347336053848, 94.16266202926636, 94.20373177528381, 94.18827766180038, 94.19992816448212, 94.27244430780411, 94.24437153339386, 94.22920596599579, 94.2226254940033, 94.25469732284546, 94.28632140159607, 94.23774665594101, 94.21859729290009, 94.21949034929276, 94.20228791236877, 94.25990253686905, 94.21967965364456, 94.25402963161469, 94.2069839835167, 94.22948914766312, 94.19182777404785, 94.23440283536911, 94.22504150867462, 94.2057837843895, 94.21559154987335, 94.26019757986069, 94.24427020549774, 94.18940234184265, 94.28867173194885, 94.208016872406, 94.22841614484787, 94.2245083451271, 94.20327043533325, 94.182861328125, 94.16633421182632, 94.27638375759125, 94.27438306808472, 94.20500749349594, 94.24416637420654, 94.22202980518341, 94.22152298688889, 94.2564651966095, 94.24157226085663, 94.25479620695114, 94.19713354110718, 94.18790221214294, 94.25490474700928, 94.18656516075134, 94.2660304903984, 94.21946960687637, 94.22519153356552, 94.22537618875504, 94.24994617700577, 94.24036782979965, 94.23942452669144, 94.27467775344849, 94.26763862371445, 94.21451556682587, 94.21226090192795, 94.24852335453033, 94.25628036260605, 94.22408801317215, 94.22275882959366, 94.20523202419281, 94.19916814565659, 94.22837167978287, 94.1497642993927, 94.22964161634445, 94.21881800889969, 94.22449618577957, 94.24588334560394, 94.22043216228485, 94.25408482551575, 94.25720971822739, 94.2073905467987]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8hiEn3K4eGB"
      },
      "source": [
        "#SAVE THE TRAINED MODEL\n",
        "MODEL_PATH = \"/content/drive/My Drive/Colab Notebooks/savedModel\"\n",
        "ENTIRE_MODEL_PATH = \"/content/drive/My Drive/Colab Notebooks/entire_model\"\n",
        "torch.save({\n",
        "    'model_state_dict':model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "}, MODEL_PATH) #overwrite to model_path\n",
        "\n",
        "torch.save(model, ENTIRE_MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVPrjKg7Rq6J",
        "outputId": "573a0e68-45e2-4a37-86f9-9d798f280ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        }
      },
      "source": [
        "#CLASSIFICATION ACCURACY MEASURE (NEW CODE)\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    claims_inp = data['claim'] #batch_size (64) X seq_size(variable)\n",
        "    sentence_inp = data['sentence'] #batch_size (64) x seq_size (variable)\n",
        "    labels = torch.tensor(data['label'], dtype=torch.long).to(device) #64\n",
        "    \n",
        "    claims_inp = getBatchIndexTensor(model, claims_inp, True).to(device)\n",
        "    sentence_inp = getBatchIndexTensor(model, sentence_inp, False).to(device)\n",
        "    \n",
        "    outputs = torch.squeeze(model(claims_inp, sentence_inp)) #outut shape : 64 x 3\n",
        "    \n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    print(\"shape = \"+str(outputs.shape))\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    print('Accuracy = '+str(100 * correct / total))\n",
        "\n",
        "print('Accuracy = '+str(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 70.3125\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 67.1875\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 68.22916666666667\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 69.53125\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 69.375\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 68.48958333333333\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 67.85714285714286\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 68.1640625\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 68.22916666666667\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 67.8125\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 68.03977272727273\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 66.40625\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 66.34615384615384\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 66.07142857142857\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 65.83333333333333\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 65.234375\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 65.25735294117646\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 65.01736111111111\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 64.96710526315789\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 65.078125\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 65.25297619047619\n",
            "shape = torch.Size([64, 3])\n",
            "Accuracy = 64.91477272727273\n",
            "shape = torch.Size([4, 3])\n",
            "Accuracy = 64.87252124645893\n",
            "Accuracy = 64.87252124645893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3SVB6Ap9s0v"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}